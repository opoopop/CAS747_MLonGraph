{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3f43cf4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_781623/572946539.py:7: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  data_org = torch.load(file_path, map_location='cpu')# data for test\n",
      "/home/chunjielu/miniconda3/envs/faiss_env/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# load data\n",
    "\n",
    "import torch\n",
    "import copy\n",
    "data_name='few_shot'\n",
    "file_path = 'data/cora_fixed_sbert.pt'\n",
    "data_org = torch.load(file_path, map_location='cpu')# data for test\n",
    "\n",
    "# org data for test\n",
    "#print(data_org.y[14])\n",
    "\n",
    "# data for train\n",
    "data_train = copy.deepcopy(data_org)  \n",
    "#print(data_train.label_names)\n",
    "all_number=2708\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1aa9ae0d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nprint(\"Train size:\", data_train.train_mask.sum().item())\\nprint(\"Val size:\", data_train.val_mask.sum().item())\\nprint(\"Test size:\", data_train.test_mask.sum().item())\\nprint(\"是否有交集:\", (data_train.train_mask & data_train.val_mask).any().item() or \\n                   (data_train.train_mask & data_train.test_mask).any().item() or \\n                   (data_train.val_mask & data_train.test_mask).any().item())\\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# randomly set train& test dataset on GNN\n",
    "\n",
    "def random_split_masks(data, num_train=140, num_val=500, num_test=1000, seed=32):\n",
    "\n",
    "    \"\"\"\n",
    "    randomly set the data for train, val, test with certain number\n",
    "    \"\"\"\n",
    "    torch.manual_seed(seed)  \n",
    "\n",
    "    num_nodes = data.num_nodes\n",
    "    all_indices = torch.randperm(num_nodes)  \n",
    "\n",
    "    # index rearrange\n",
    "    train_idx = all_indices[:num_train]\n",
    "    val_idx = all_indices[num_train:num_train + num_val]\n",
    "    test_idx = all_indices[num_train + num_val:num_train + num_val + num_test]\n",
    "\n",
    "\n",
    "    data.train_mask = torch.zeros(num_nodes, dtype=torch.bool)\n",
    "    data.val_mask = torch.zeros(num_nodes, dtype=torch.bool)\n",
    "    data.test_mask = torch.zeros(num_nodes, dtype=torch.bool)\n",
    "\n",
    "    data.train_mask[train_idx] = True\n",
    "    data.val_mask[val_idx] = True\n",
    "    data.test_mask[test_idx] = True\n",
    "\n",
    "    return data \n",
    "\n",
    "data_org = random_split_masks(data_org)\n",
    "data_train = random_split_masks(data_train)\n",
    "\n",
    "\"\"\"\n",
    "print(\"Train size:\", data_train.train_mask.sum().item())\n",
    "print(\"Val size:\", data_train.val_mask.sum().item())\n",
    "print(\"Test size:\", data_train.test_mask.sum().item())\n",
    "print(\"是否有交集:\", (data_train.train_mask & data_train.val_mask).any().item() or \n",
    "                   (data_train.train_mask & data_train.test_mask).any().item() or \n",
    "                   (data_train.val_mask & data_train.test_mask).any().item())\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dd2c1298",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_781623/2198844873.py:5: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  data = torch.load(file_path, map_location='cpu')\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# load data\n",
    "file_path = 'LLM_GNN_data/cora_openai.pt'  \n",
    "data = torch.load(file_path, map_location='cpu')\n",
    "\n",
    "\n",
    "consistency_data = data[data_name]\n",
    "class_map = { # a mapping from the responses\n",
    "    'rule_Learning': 0,\n",
    "    'neural_networks': 1,\n",
    "    \"case_based\": 2,\n",
    "    'genetic_algorithms': 3,\n",
    "    'theory': 4,\n",
    "    \"reinforcement_learning\": 5,\n",
    "    \"probabilistic_methods\": 6\n",
    "}\n",
    "#print(consistency_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2e5f7752",
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "from torch_geometric.datasets import Planetoid\n",
    "from torch_geometric.utils import to_networkx\n",
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "from torch_geometric.datasets import Planetoid\n",
    "from sklearn.metrics.pairwise import euclidean_distances\n",
    "import faiss\n",
    "\n",
    "def get_rank_score(rank_list,n,m):\n",
    "    result=[0 for i in range(n)]\n",
    "    for i in range(m):\n",
    "        k=rank_list[i]\n",
    "        result[k]=100*(m-i)/m\n",
    "    return result\n",
    "def compute_rC_density(node_features: torch.Tensor, cluster_centers: torch.Tensor, labels: torch.Tensor):\n",
    "    \"\"\"\n",
    "    cal the C-density score\n",
    "\n",
    "        node_features: [N, D] node feature\n",
    "        cluster_centers: [K, D] cluster centure\n",
    "        labels: [N] the label\n",
    "\n",
    "    \"\"\"\n",
    "    assigned_centers = cluster_centers[labels]        \n",
    "    distances = torch.norm(node_features - assigned_centers, dim=1)  \n",
    "    c_density = 1 / (1 + distances)  \n",
    "    return c_density\n",
    "\n",
    "\n",
    "def compute_cluster_rank(data, num_clusters=7):\n",
    "    \"\"\"\n",
    "    implement k-means then get c_density score , then return the ranking\n",
    "    \"\"\"\n",
    "\n",
    "    x = data.x.detach().cpu().float()\n",
    "    n, d = x.shape\n",
    "\n",
    "    kmeans = faiss.Kmeans(d=d, k=num_clusters, niter=20, verbose=False, seed=42)\n",
    "    kmeans.train(x.numpy())\n",
    "\n",
    "\n",
    "    _, I = kmeans.index.search(x.numpy(), 1)  # I: [N, 1]\n",
    "    labels = torch.tensor(I.squeeze(), dtype=torch.long)  # [N]\n",
    "\n",
    "    centers = torch.tensor(kmeans.centroids, dtype=x.dtype)  # [K, D]\n",
    "    c_density = compute_rC_density(x, centers, labels)  # [N]\n",
    "    sorted_idx = torch.argsort(c_density, descending=True)  # [N]\n",
    "\n",
    "    rank_list = sorted_idx.tolist()\n",
    "    return get_rank_score(rank_list, data.num_nodes,data.num_nodes)\n",
    "\n",
    "\n",
    "cluster_list=compute_cluster_rank(data_train)\n",
    "#print(cluster_list)\n",
    "# get R_c-density for every node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9a37ad2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "annotation accuracy 0.7097966728280961 with 541 nodes\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# process the responses (string)\n",
    "unknown_labels = set()\n",
    "\n",
    "def get_result(s):\n",
    "    \"\"\"\n",
    "    return the class [0,7) and it's confidence score from the string returned by LLM\n",
    "    \"\"\"\n",
    "    l=0\n",
    "    while(s[l]!=\":\"):\n",
    "        l+=1\n",
    "    l+=2\n",
    "    if s[l-1]!='\"':\n",
    "        l+=1\n",
    "\n",
    "    r=l\n",
    "    while(s[r]!='\"'):\n",
    "        r+=1\n",
    "    label_str=s[l:r]\n",
    "    #print(label_str)\n",
    "    conf=0\n",
    "    for i in s:\n",
    "        if ord(i)>=ord('0') and ord(i)<=ord('9'):\n",
    "            conf=conf*10+ord(i)-ord('0')\n",
    "\n",
    "    if label_str not in class_map:\n",
    "        unknown_labels.add(label_str)\n",
    "    \n",
    "    else:\n",
    "        return class_map[label_str],conf\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "# result store the value we need (index,(prediction, confidence score))\n",
    "result=[]\n",
    "for i, item in enumerate(consistency_data):\n",
    "    if not item or not isinstance(item, list) or not any(s.strip() for s in item):\n",
    "        continue\n",
    "    if get_result(item[0])!=None:\n",
    "        result.append([i,get_result(item[0])])\n",
    "    #print(i,get_result(item[0]))\n",
    "\n",
    "#print(result)\n",
    "\n",
    "total=0\n",
    "op=0\n",
    "mask_list=[] # mask list to give a train mask\n",
    "label_list=[]# a list of label with order of mask_list\n",
    "anao=[-1 for i in range(len(consistency_data))]\n",
    "# all 2078 annotations, -1 means it has on response form openai \n",
    "conf_list=[]\n",
    "for i in range(len(result)):\n",
    "    idx=result[i][0]\n",
    "    anao[idx]=result[i][1][1]/100\n",
    "    conf_list.append(result[i][1][1]/100)\n",
    "    label=result[i][1][0]\n",
    "    mask_list.append(idx)\n",
    "    label_list.append(label)\n",
    "    if data_org.y[idx].item()==label:\n",
    "        op+=1\n",
    "    total+=1\n",
    "#print(op/total,total)\n",
    "print(f'annotation accuracy {op/total} with {total} nodes')\n",
    "# turn into tensor\n",
    "mask_list_tensor=torch.tensor(mask_list)\n",
    "label_list_tensor=torch.tensor(label_list)\n",
    "annotations=torch.tensor(anao)\n",
    "\n",
    "\n",
    "#print(mask_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "18700abf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'    \\n# conf rank and coe rank\\nconf_rank=get_confidence_rank_score(mask_list,conf_list)\\nprint(conf_rank)\\nrank=compute_rCOE(mask_list,annotations)\\nprint(len(rank))\\n\\nprint(conf_rank)\\nfor i in conf_rank:\\n    print(anao[i])\\n'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from collections import Counter\n",
    "from scipy.stats import entropy\n",
    "def compute_entropy_from_labels(labels):\n",
    "    \"\"\"\n",
    "    cal  Shannon entropy\n",
    "    labels: List[int] or 1D tensor\n",
    "    \"\"\"\n",
    "    labels = labels.cpu().numpy() if isinstance(labels, torch.Tensor) else np.array(labels)\n",
    "    counter = Counter(labels)\n",
    "    probs = np.array([v / len(labels) for v in counter.values()])\n",
    "    return entropy(probs, base=np.e)  # Shannon entropy in natural log base\n",
    "\n",
    "\n",
    "def compute_rCOE(selected_nodes, annotations, return_rank=True):\n",
    "    \"\"\"\n",
    "    cal every node's COE score return ranking\n",
    "    \"\"\"\n",
    "    if isinstance(selected_nodes, torch.Tensor):\n",
    "        selected_nodes = selected_nodes.tolist()\n",
    "\n",
    "    selected_labels = annotations[selected_nodes]\n",
    "    base_entropy = compute_entropy_from_labels(selected_labels)\n",
    "\n",
    "    coe_scores = {}\n",
    "    for node in selected_nodes:\n",
    "        # subset apart from one node\n",
    "        remaining_nodes = [n for n in selected_nodes if n != node]\n",
    "        remaining_labels = annotations[remaining_nodes]\n",
    "        new_entropy = compute_entropy_from_labels(remaining_labels)\n",
    "        coe_scores[node] = new_entropy - base_entropy\n",
    "\n",
    "    if return_rank:\n",
    "        sorted_nodes = sorted(coe_scores.items(), key=lambda x: x[1], reverse=True)\n",
    "        ranked_nodes = [node for node, _ in sorted_nodes]\n",
    "        return get_rank_score(ranked_nodes,all_number,len(ranked_nodes))\n",
    "\n",
    "    return coe_scores\n",
    "\n",
    "def get_rank_score(rank_list,n,m):\n",
    "    result=[-1 for i in range(n)]\n",
    "    for i in range(m):\n",
    "        k=rank_list[i]\n",
    "        result[k]=100*(m-i)/m\n",
    "    return result\n",
    "\n",
    "def get_confidence_rank_score(selected_nodes,conf):\n",
    "    result=[]\n",
    "    for i in range(len(selected_nodes)):\n",
    "        result.append((selected_nodes[i],conf[i]))\n",
    "\n",
    "    sorted_result=sorted(result, key=lambda x: x[1], reverse=True)\n",
    "    rank_list = [node for node, _ in sorted_result]\n",
    "    #print(len(rank_list))\n",
    "    score_list=get_rank_score(rank_list,all_number,len(rank_list))\n",
    "\n",
    "    return score_list\n",
    "\"\"\"    \n",
    "# conf rank and coe rank\n",
    "conf_rank=get_confidence_rank_score(mask_list,conf_list)\n",
    "print(conf_rank)\n",
    "rank=compute_rCOE(mask_list,annotations)\n",
    "print(len(rank))\n",
    "\n",
    "print(conf_rank)\n",
    "for i in conf_rank:\n",
    "    print(anao[i])\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cf041f02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "process 40 nodes\n",
      "process 80 nodes\n",
      "process 120 nodes\n",
      "process 160 nodes\n",
      "process 200 nodes\n",
      "process 240 nodes\n",
      "300\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def get_filter_out_index(mask_list,label_list,conf_list,annotations,cluster_list):\n",
    "    \"\"\"\n",
    "    get score= c-density-score+cond_score+COE_score\n",
    "    \"\"\"\n",
    "    conf_score=get_confidence_rank_score(mask_list,conf_list)\n",
    "    COE_score=compute_rCOE(mask_list,annotations)\n",
    "\n",
    "    op=-1\n",
    "    val=-1\n",
    "    for i in range(len(COE_score)):\n",
    "        if conf_score[i]==-1:\n",
    "            continue\n",
    "        score=0.25*conf_score[i]+0.5*COE_score[i]+0.25*cluster_list[i]\n",
    "        if op==-1 or score<val:\n",
    "            val=score\n",
    "            op=i\n",
    "    return op\n",
    "\n",
    "\n",
    "def post_filter(final_number,mask_list,label_list,conf_list,annotations,cluster_list):\n",
    "    \"\"\"\n",
    "    return the list with the certain number(final_number) [mask_list,label_list]\n",
    "    using score= c-density-score+cond_score+COE_score\n",
    "\n",
    "    \"\"\"\n",
    "    process_num=0\n",
    "    if final_number>=len(mask_list):\n",
    "        print('No nodes will be filtered out')\n",
    "        return [mask_list,label_list]\n",
    "    for k in range(len(mask_list)-final_number):\n",
    "        process_num+=1\n",
    "        j=get_filter_out_index(mask_list,label_list,conf_list,annotations,cluster_list)\n",
    "        if process_num%40==0:\n",
    "            print(f'process {process_num} nodes')\n",
    "        #print(j)\n",
    "        new_label_list=[]\n",
    "        new_conf_list=[]\n",
    "        new_mask_list=[]\n",
    "        for i in range(len(mask_list)):\n",
    "            if mask_list[i]==j:\n",
    "                #print(mask_list[i],j)\n",
    "                continue\n",
    "            new_label_list.append(label_list[i])\n",
    "            new_conf_list.append(conf_list[i])\n",
    "            new_mask_list.append(mask_list[i])\n",
    "        \n",
    "        mask_list = new_mask_list\n",
    "        label_list = new_label_list\n",
    "        conf_list = new_conf_list\n",
    "    \n",
    "    return [mask_list,label_list]\n",
    "\n",
    "\n",
    "A,B=post_filter(300,mask_list,label_list,conf_list,annotations,cluster_list)\n",
    "\n",
    "            \n",
    "print(len(A))\n",
    "#print(len(B))\n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7ef2e12b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training nodes: 300\n"
     ]
    }
   ],
   "source": [
    "def change_trainmask_and_label(mask_list,label_list,data):\n",
    "    train_mask = torch.zeros(data_train.num_nodes, dtype=torch.bool)\n",
    "    train_mask[mask_list] = True\n",
    "    data.train_mask = train_mask\n",
    "    data.y[data_train.train_mask] = label_list\n",
    "\n",
    "\n",
    "change_trainmask_and_label(A,torch.tensor(B),data_train)\n",
    "\n",
    "train_nodes = data_train.train_mask.sum().item()  \n",
    "print(f'Number of training nodes: {train_nodes}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bbca9e62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 001, Loss: 2.3452, Train: 0.1433, Val: 0.1440\n",
      "Epoch 020, Loss: 0.6369, Train: 0.8367, Val: 0.6840\n",
      "Epoch 040, Loss: 0.4575, Train: 0.8800, Val: 0.6940\n",
      "Epoch 060, Loss: 0.3784, Train: 0.9067, Val: 0.7160\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv\n",
    "class GCN2(torch.nn.Module):\n",
    "    def __init__(self, num_layers, input_dim, hidden_dimension, num_classes, dropout, norm=None) -> None:\n",
    "        super().__init__()\n",
    "        self.convs = torch.nn.ModuleList()\n",
    "        self.norms = torch.nn.ModuleList()\n",
    "        self.num_layers = num_layers\n",
    "        self.dropout = dropout\n",
    "        if num_layers == 1:\n",
    "            self.convs.append(GCNConv(input_dim, num_classes, cached=False,\n",
    "                             normalize=True))\n",
    "        else:\n",
    "            self.convs.append(GCNConv(input_dim, hidden_dimension, cached=False,\n",
    "                             normalize=True))\n",
    "            if norm:\n",
    "                self.norms.append(torch.nn.BatchNorm1d(hidden_dimension))\n",
    "            else:\n",
    "                self.norms.append(torch.nn.Identity())\n",
    "\n",
    "            for _ in range(num_layers - 2):\n",
    "                self.convs.append(GCNConv(hidden_dimension, hidden_dimension, cached=False,\n",
    "                             normalize=True))\n",
    "                if norm:\n",
    "                    self.norms.append(torch.nn.BatchNorm1d(hidden_dimension))\n",
    "                else:\n",
    "                    self.norms.append(torch.nn.Identity())\n",
    "\n",
    "            self.convs.append(GCNConv(hidden_dimension, num_classes, cached=False, normalize=True))\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index, edge_weight= data.x, data.edge_index, data.edge_weight\n",
    "        for i in range(self.num_layers):\n",
    "            x = F.dropout(x, p=self.dropout, training=self.training)\n",
    "            if edge_weight != None:\n",
    "                x = self.convs[i](x, edge_index, edge_weight)\n",
    "            else:\n",
    "                x = self.convs[i](x, edge_index)\n",
    "            if i != self.num_layers - 1:\n",
    "                x = self.norms[i](x)\n",
    "                x = F.relu(x)\n",
    "        return x\n",
    "\n",
    "model = GCN2(num_layers=2, input_dim=data_train.num_node_features, hidden_dimension=64, \n",
    "            num_classes=7, dropout=0.5, norm=True)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=5e-4)\n",
    "\n",
    "# 训练函数\n",
    "def train(data):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    out = model(data)\n",
    "    loss = F.cross_entropy(out[data.train_mask], data.y[data.train_mask])\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return loss.item()\n",
    "\n",
    "def test(data):\n",
    "    model.eval()\n",
    "    out = model(data)\n",
    "    pred = out.argmax(dim=1)\n",
    "    accs = []\n",
    "    for mask in [data.train_mask, data.val_mask]:\n",
    "        correct = pred[mask].eq(data.y[mask]).sum().item()\n",
    "        accs.append(correct / mask.sum().item())\n",
    "    return accs  # train_acc, val_acc, test_acc\n",
    "def test_final(data):\n",
    "    model.eval()\n",
    "    out = model(data)\n",
    "    pred = out.argmax(dim=1)\n",
    "    accs = []\n",
    "    for mask in [data.test_mask]:\n",
    "        correct = pred[mask].eq(data.y[mask]).sum().item()\n",
    "        accs.append(correct / mask.sum().item())\n",
    "    return accs  # train_acc, val_acc, test_acc\n",
    "\n",
    "# 训练流程\n",
    "for epoch in range(1, 80):\n",
    "    loss = train(data_train)\n",
    "    train_acc, val_acc = test(data_train)\n",
    "    if epoch % 20 == 0 or epoch == 1:\n",
    "        print(f\"Epoch {epoch:03d}, Loss: {loss:.4f}, Train: {train_acc:.4f}, Val: {val_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5d52fcdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "final accuracy: [0.742]\n"
     ]
    }
   ],
   "source": [
    "print(f'final accuracy: {test_final(data_org)}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
