{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d1ffd09c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_781528/528042771.py:7: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  data_org = torch.load(file_path, map_location='cpu')# data for test\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Agents', 'ML', 'IR', 'DB', 'HCI', 'AI']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chunjielu/miniconda3/envs/faiss_env/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# load data\n",
    "\n",
    "import torch\n",
    "import copy\n",
    "data_name='topk'\n",
    "file_path = 'data/citeseer_fixed_sbert.pt'\n",
    "data_org = torch.load(file_path, map_location='cpu')# data for test\n",
    "\n",
    "# org data for test\n",
    "#print(data_org.y[14])\n",
    "print(data_org.label_names)\n",
    "# data for train\n",
    "data_train = copy.deepcopy(data_org)  \n",
    "#print(data_train.label_names)\n",
    "all_number=3186\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cc40c580",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nprint(\"Train size:\", data_train.train_mask.sum().item())\\nprint(\"Val size:\", data_train.val_mask.sum().item())\\nprint(\"Test size:\", data_train.test_mask.sum().item())\\nprint(\"是否有交集:\", (data_train.train_mask & data_train.val_mask).any().item() or \\n                   (data_train.train_mask & data_train.test_mask).any().item() or \\n                   (data_train.val_mask & data_train.test_mask).any().item())\\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# randomly set train& test dataset on GNN\n",
    "\n",
    "def random_split_masks(data, num_train=120, num_val=500, num_test=1000, seed=42):\n",
    "\n",
    "    \"\"\"\n",
    "    randomly set the data for train, val, test with certain number\n",
    "    \"\"\"\n",
    "    torch.manual_seed(seed)  \n",
    "\n",
    "    num_nodes = data.num_nodes\n",
    "    all_indices = torch.randperm(num_nodes)  \n",
    "\n",
    "    # index rearrange\n",
    "    train_idx = all_indices[:num_train]\n",
    "    val_idx = all_indices[num_train:num_train + num_val]\n",
    "    test_idx = all_indices[num_train + num_val:num_train + num_val + num_test]\n",
    "\n",
    "\n",
    "    data.train_mask = torch.zeros(num_nodes, dtype=torch.bool)\n",
    "    data.val_mask = torch.zeros(num_nodes, dtype=torch.bool)\n",
    "    data.test_mask = torch.zeros(num_nodes, dtype=torch.bool)\n",
    "\n",
    "    data.train_mask[train_idx] = True\n",
    "    data.val_mask[val_idx] = True\n",
    "    data.test_mask[test_idx] = True\n",
    "\n",
    "    return data \n",
    "\n",
    "data_org = random_split_masks(data_org)\n",
    "data_train = random_split_masks(data_train)\n",
    "\n",
    "\"\"\"\n",
    "print(\"Train size:\", data_train.train_mask.sum().item())\n",
    "print(\"Val size:\", data_train.val_mask.sum().item())\n",
    "print(\"Test size:\", data_train.test_mask.sum().item())\n",
    "print(\"是否有交集:\", (data_train.train_mask & data_train.val_mask).any().item() or \n",
    "                   (data_train.train_mask & data_train.test_mask).any().item() or \n",
    "                   (data_train.val_mask & data_train.test_mask).any().item())\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "362536b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_781528/3637104084.py:5: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  data = torch.load(file_path, map_location='cpu')\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# load data\n",
    "file_path = 'LLM_GNN_data/citeseer_openai.pt'  \n",
    "data = torch.load(file_path, map_location='cpu')\n",
    "\n",
    "\n",
    "consistency_data = data[data_name]\n",
    "class_map = { # a mapping from the responses\n",
    "    'Agents': 0,\n",
    "    'machine learning': 1,\n",
    "    \"information retrieval\": 2,\n",
    "    'database': 3,\n",
    "    'human computer interaction': 4,\n",
    "    \"artificial intelligence\": 5\n",
    "\n",
    "}\n",
    "#print(consistency_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "17fee746",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy of all the annotations: 0.609504132231405\n",
      "{'computer vision', 'software development', 'Multi-Agent Systems', 'control', 'agents'}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# process the responses (string)\n",
    "unknown_labels = set()\n",
    "\n",
    "def get_result(s):\n",
    "    #print(s)\n",
    "    \"\"\"\n",
    "    return the class [0,7) and it's confidence score from the string returned by LLM\n",
    "    \"\"\"\n",
    "    l=0\n",
    "    while(s[l]!=\":\"):\n",
    "        l+=1\n",
    "    l+=2\n",
    "    if s[l-1]!='\"' and s[l-1]!=\"'\":\n",
    "        l+=1\n",
    "\n",
    "    r=l\n",
    "    #print(r)\n",
    "    while(s[r]!='\"' and s[r]!=\"'\"):\n",
    "        r+=1\n",
    "    label_str=s[l:r]\n",
    "    #print(label_str)\n",
    "    conf=0\n",
    "    for i in s:\n",
    "        if ord(i)>=ord('0') and ord(i)<=ord('9'):\n",
    "            conf=conf*10+ord(i)-ord('0')\n",
    "\n",
    "    if label_str not in class_map:\n",
    "        unknown_labels.add(label_str)\n",
    "    \n",
    "    else:\n",
    "        return class_map[label_str],conf\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "result=[] # (index, (label, confidecnce score))\n",
    "for i, item in enumerate(consistency_data):\n",
    "    if not item or not isinstance(item, list) or not any(s.strip() for s in item):\n",
    "        continue\n",
    "    if get_result(item[0])!=None:\n",
    "        result.append([i,get_result(item[0])])\n",
    "    #print(i,get_result(item[0]))\n",
    "\n",
    "#print(result)\n",
    "total=0\n",
    "op=0\n",
    "\n",
    "mask_list=[]\n",
    "label_list=[]\n",
    "anao=[0 for i in range(len(consistency_data))] \n",
    "\n",
    "for i in range(len(result)):\n",
    "    idx=result[i][0]\n",
    "    anao[idx]=result[i][1][1]/100\n",
    "    label=result[i][1][0]\n",
    "    mask_list.append(idx)\n",
    "    label_list.append(label)\n",
    "    if data_org.y[idx].item()==label:\n",
    "        op+=1\n",
    "    total+=1\n",
    "print(f'accuracy of all the annotations: {op/total}') # the accuracy of response compared with the ground truth\n",
    "\n",
    "annotations=torch.tensor(anao)\n",
    "mask_list_tensor=torch.tensor(mask_list)\n",
    "label_list_tensor=torch.tensor(label_list)\n",
    "print(unknown_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d6b56198",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the training mask and label into the responses'data\n",
    "def change_trainmask_and_label(mask_list,label_list,data):\n",
    "    train_mask = torch.zeros(data_train.num_nodes, dtype=torch.bool)\n",
    "    train_mask[mask_list] = True\n",
    "    data.train_mask = train_mask\n",
    "    data.y[data_train.train_mask] = label_list\n",
    "\n",
    "change_trainmask_and_label(mask_list_tensor,label_list_tensor,data_train)\n",
    "\n",
    "train_nodes = data_train.train_mask.sum().item() \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a02f383a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 001, Loss: 2.3848, Train: 0.2169, Val: 0.2220\n",
      "Epoch 020, Loss: 1.0587, Train: 0.8368, Val: 0.5640\n",
      "Epoch 040, Loss: 0.6727, Train: 0.8843, Val: 0.5800\n",
      "Epoch 060, Loss: 0.5656, Train: 0.9091, Val: 0.5860\n",
      "Epoch 080, Loss: 0.4771, Train: 0.9339, Val: 0.5960\n",
      "Epoch 100, Loss: 0.4263, Train: 0.9442, Val: 0.5940\n",
      "Epoch 120, Loss: 0.3310, Train: 0.9649, Val: 0.6000\n",
      "Epoch 140, Loss: 0.3546, Train: 0.9752, Val: 0.6040\n",
      "Epoch 160, Loss: 0.3297, Train: 0.9814, Val: 0.6040\n",
      "Epoch 180, Loss: 0.2734, Train: 0.9855, Val: 0.5980\n",
      "Epoch 200, Loss: 0.2959, Train: 0.9938, Val: 0.6040\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv\n",
    "class GCN2(torch.nn.Module):\n",
    "    def __init__(self, num_layers, input_dim, hidden_dimension, num_classes, dropout, norm=None) -> None:\n",
    "        super().__init__()\n",
    "        self.convs = torch.nn.ModuleList()\n",
    "        self.norms = torch.nn.ModuleList()\n",
    "        self.num_layers = num_layers\n",
    "        self.dropout = dropout\n",
    "        if num_layers == 1:\n",
    "            self.convs.append(GCNConv(input_dim, num_classes, cached=False,\n",
    "                             normalize=True))\n",
    "        else:\n",
    "            self.convs.append(GCNConv(input_dim, hidden_dimension, cached=False,\n",
    "                             normalize=True))\n",
    "            if norm:\n",
    "                self.norms.append(torch.nn.BatchNorm1d(hidden_dimension))\n",
    "            else:\n",
    "                self.norms.append(torch.nn.Identity())\n",
    "\n",
    "            for _ in range(num_layers - 2):\n",
    "                self.convs.append(GCNConv(hidden_dimension, hidden_dimension, cached=False,\n",
    "                             normalize=True))\n",
    "                if norm:\n",
    "                    self.norms.append(torch.nn.BatchNorm1d(hidden_dimension))\n",
    "                else:\n",
    "                    self.norms.append(torch.nn.Identity())\n",
    "\n",
    "            self.convs.append(GCNConv(hidden_dimension, num_classes, cached=False, normalize=True))\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index, edge_weight= data.x, data.edge_index, data.edge_weight\n",
    "        for i in range(self.num_layers):\n",
    "            x = F.dropout(x, p=self.dropout, training=self.training)\n",
    "            if edge_weight != None:\n",
    "                x = self.convs[i](x, edge_index, edge_weight)\n",
    "            else:\n",
    "                x = self.convs[i](x, edge_index)\n",
    "            if i != self.num_layers - 1:\n",
    "                x = self.norms[i](x)\n",
    "                x = F.relu(x)\n",
    "        return x\n",
    "\n",
    "model = GCN2(num_layers=2, input_dim=data_train.num_node_features, hidden_dimension=64, \n",
    "            num_classes=6, dropout=0.5, norm=True)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=5e-4)\n",
    "\n",
    "# 训练函数\n",
    "def train(data):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    out = model(data)\n",
    "    loss = F.cross_entropy(out[data.train_mask], data.y[data.train_mask])\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return loss.item()\n",
    "\n",
    "# 测试函数\n",
    "def test(data):\n",
    "    model.eval()\n",
    "    out = model(data)\n",
    "    pred = out.argmax(dim=1)\n",
    "    accs = []\n",
    "    for mask in [data.train_mask, data.val_mask]:\n",
    "        correct = pred[mask].eq(data.y[mask]).sum().item()\n",
    "        accs.append(correct / mask.sum().item())\n",
    "    return accs  # train_acc, val_acc, test_acc\n",
    "def test_final(data):\n",
    "    model.eval()\n",
    "    out = model(data)\n",
    "    pred = out.argmax(dim=1)\n",
    "    accs = []\n",
    "    for mask in [data.test_mask]:\n",
    "        correct = pred[mask].eq(data.y[mask]).sum().item()\n",
    "        accs.append(correct / mask.sum().item())\n",
    "    return accs  # train_acc, val_acc, test_acc\n",
    "\n",
    "# 训练流程\n",
    "for epoch in range(1, 201):\n",
    "    loss = train(data_train)\n",
    "    train_acc, val_acc = test(data_train)\n",
    "    if epoch % 20 == 0 or epoch == 1:\n",
    "        print(f\"Epoch {epoch:03d}, Loss: {loss:.4f}, Train: {train_acc:.4f}, Val: {val_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "98cbc0fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training nodes: 484\n",
      "final accuracy: [0.59]\n"
     ]
    }
   ],
   "source": [
    "print(f'Number of training nodes: {train_nodes}')\n",
    "print(f'final accuracy: {test_final(data_org)}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
