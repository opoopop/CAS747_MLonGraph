{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_782486/765789076.py:7: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  data_org = torch.load(file_path, map_location='cpu')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Rule_Learning', 'Neural_Networks', 'Case_Based', 'Genetic_Algorithms', 'Theory', 'Reinforcement_Learning', 'Probabilistic_Methods']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chunjielu/miniconda3/envs/faiss_env/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# loading data\n",
    "\n",
    "import torch\n",
    "import copy\n",
    "\n",
    "file_path = 'data/cora_fixed_sbert.pt'\n",
    "data_org = torch.load(file_path, map_location='cpu')\n",
    "\n",
    "# org data for test\n",
    "#print(data_org.y[14])\n",
    "\n",
    "# data for train\n",
    "data_train = copy.deepcopy(data_org)  \n",
    "print(data_train.label_names)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy 0.8918918918918919, number 37\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_782486/2059984175.py:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  LLM_label = torch.load(file_path, map_location='cpu')\n"
     ]
    }
   ],
   "source": [
    "file_path = 'LLM_GNN_data/cora^cache^consistency.pt'\n",
    "LLM_label = torch.load(file_path, map_location='cpu')\n",
    "op=0\n",
    "total=0\n",
    "for i in range(2708):\n",
    "    if LLM_label['conf'][i].item()<0.90:\n",
    "        continue\n",
    "    total+=1\n",
    "    if LLM_label['pred'][i].item()==data_org.y[i].item():\n",
    "        op+=1\n",
    "        #print(i,LLM_label['conf'][i].item())\n",
    "#print(op/total , total)\n",
    "print(f'accuracy {op/total}, number {total}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size: 140\n",
      "Val size: 500\n",
      "Test size: 1000\n",
      "是否有交集: False\n"
     ]
    }
   ],
   "source": [
    "# set the test_data for test\n",
    "\n",
    "def random_split_masks(data, num_train=140, num_val=500, num_test=1000, seed=42):\n",
    "    torch.manual_seed(seed)  \n",
    "\n",
    "    num_nodes = data.num_nodes\n",
    "    all_indices = torch.randperm(num_nodes) \n",
    "\n",
    "\n",
    "    train_idx = all_indices[:num_train]\n",
    "    val_idx = all_indices[num_train:num_train + num_val]\n",
    "    test_idx = all_indices[num_train + num_val:num_train + num_val + num_test]\n",
    "\n",
    "    data.train_mask = torch.zeros(num_nodes, dtype=torch.bool)\n",
    "    data.val_mask = torch.zeros(num_nodes, dtype=torch.bool)\n",
    "    data.test_mask = torch.zeros(num_nodes, dtype=torch.bool)\n",
    "\n",
    "    data.train_mask[train_idx] = True\n",
    "    data.val_mask[val_idx] = True\n",
    "    data.test_mask[test_idx] = True\n",
    "\n",
    "    return data \n",
    "data_org = random_split_masks(data_org)\n",
    "data_train = random_split_masks(data_train)\n",
    "\n",
    "print(\"Train size:\", data_train.train_mask.sum().item())\n",
    "print(\"Val size:\", data_train.val_mask.sum().item())\n",
    "print(\"Test size:\", data_train.test_mask.sum().item())\n",
    "print(\"是否有交集:\", (data_train.train_mask & data_train.val_mask).any().item() or \n",
    "                   (data_train.train_mask & data_train.test_mask).any().item() or \n",
    "                   (data_train.val_mask & data_train.test_mask).any().item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2178, 1686, 2583, 753, 31, 1982, 1880, 1896, 2036, 1548, 1516, 2406, 2159, 1864, 1634, 2065, 2167, 1554, 927, 2467, 962, 2175, 1163, 1214, 2614, 949, 2117, 312, 1164, 1633, 1595, 1642, 893, 1627, 990, 1390, 1507, 1981, 2174, 2215, 1118, 1462, 2216, 2559, 1528, 1983, 2439, 1070, 1637, 1269, 1536, 2264, 1053, 1857, 56, 1260, 2177, 1372, 58, 1635, 1369, 964, 1047, 1171, 1948, 1020, 921, 2571, 2612, 961, 2058, 16, 1499, 1610, 1701, 1620, 1569, 449, 2014, 2010, 2513, 68, 1052, 11, 2654, 997, 1584, 1914, 2376, 2481, 1416, 1762, 1953, 1840, 2688, 2340, 940, 1588, 2300, 2099, 2173, 69, 2530, 1711, 2105, 1440, 2183, 2650, 1998, 2186, 1892, 2628, 1761, 1270, 1578, 1207, 1524, 178, 1671, 1741, 699, 1134, 1263, 2348, 1413, 977, 1927, 1932, 2343, 1011, 1926, 2181, 889, 2381, 2082, 1005, 1592, 1665, 1791, 2123, 1717, 2492, 1437, 1626, 1763, 2627, 2635, 1593, 1267, 933, 1344, 1764, 2304, 1153, 1629, 2043, 1147, 1604, 2164, 2253, 2613, 2333, 2278, 1787, 1863, 0, 2676, 1558, 446, 1101, 1653, 1697, 1749, 2339, 1358, 2050, 1420, 1016, 2541, 986, 1148, 2319, 2265, 1497, 2653, 2369, 1564, 1999, 2169, 1920, 947, 409, 1515, 30, 2480, 1039, 1504, 945, 970, 1149, 1834, 1705, 2539, 2610, 1124, 1621, 1312, 1788, 1178, 1658, 192, 1018, 2463, 2588, 536, 1643, 778, 1286, 112, 2166, 1630, 2341, 892, 1674, 2624, 885, 2196, 2690, 2648, 2409, 1465, 917, 1065, 2595, 1317, 290, 1915, 2693, 1511, 2611, 67, 1275, 393, 1017, 1295, 2239, 1631, 2318, 150, 2133, 299, 963, 1272, 529, 944, 43, 1380, 989, 633, 2668, 1666, 1725, 265, 2385, 1744, 324, 62, 2136, 1894, 1512, 1933, 2090, 543, 2644, 597, 1141, 163, 2226, 378, 923, 1463, 331, 2383, 1821, 1025, 1638, 1399, 1432, 2363, 2135, 506, 1252, 2600, 1879, 2377, 938, 2566, 1221, 1401, 2696, 1813, 1916, 2071, 2184, 1217, 2692, 926, 2362, 984, 158, 38, 1557, 2223, 1699, 1409, 1836, 1655, 1997, 13, 2411, 1215, 1339, 1551, 145, 998, 1964, 2255, 2563, 1103, 1733, 2059, 32, 1254, 2616, 243, 831, 1225, 2054, 696, 1135, 890, 2230, 2252, 930, 1444, 2407, 913, 2408, 1294, 159, 74, 832, 1755, 1489, 475, 1708, 2179, 2459, 2673, 1972, 1138, 1122, 2680, 1407, 2234, 61, 1058, 980, 2674, 2257, 1311, 1859, 2699, 1094, 1032, 2591, 2137, 1245, 273, 1208, 204, 301, 464, 1003, 1615, 2206, 2412, 1624, 1678, 1131, 2101, 2326, 1082, 1470, 1485, 2546, 1279, 1720, 1753, 1408, 1775, 1226, 1203, 2380, 201, 276, 2062, 2100, 376, 1532, 1993, 2545, 839, 2677, 1522, 373, 2303, 2378, 184, 1366, 2666, 87, 2001, 848, 1433, 1268, 1517, 1533, 1387, 2643, 937, 1271, 2608, 1673, 2203, 1310, 1262, 2284, 59, 781, 830, 877, 1422, 1641, 1338, 709, 1019, 1555, 1991, 2037, 2342, 1029, 1461, 1766, 245, 2152, 1100, 598, 1934, 1690, 1296, 461, 1426, 2214, 939, 1036, 2620, 1820, 2601, 2115, 1868, 1768, 2633, 1973, 196, 442, 1979, 2366, 1347, 1396, 2410, 1986, 1659, 1681, 966, 2127, 1526, 2294, 1995, 15, 1128, 1180, 936, 1765, 1835, 1292, 2457, 279, 481, 1957, 1012, 509, 1022, 1492, 2649, 2030, 2655, 891, 1579, 2575, 1808, 1968, 1698, 1364, 2192, 1565, 1571, 1707, 1809, 2079, 2685, 1870, 1962, 1529, 1781, 1888, 974, 2238, 496, 2189, 1495, 1314, 2061, 2454, 1530, 2069, 969, 1843, 2424, 1379, 2678, 899, 2235, 884, 1873, 2093, 1237, 517, 1479, 2045, 2024, 1929, 2540, 1928, 44, 1909, 175, 1559, 1714, 1589, 2434, 1293, 155, 79, 2193, 993, 1586, 1987, 1220, 2415, 2681, 2352, 436, 2261, 183, 1647, 1218, 2705, 53, 1, 1760, 131, 1068, 534, 5, 1556, 160, 8, 898, 172, 565, 123, 141, 2630, 2461, 2194, 2525, 1084, 190, 1612, 2368, 808, 2570, 36, 35, 2094, 2373, 388, 463, 1266, 249, 2243, 1476, 1484, 221, 2081, 2328, 1491, 1352, 1436, 1115, 1323, 1572, 614, 1035, 2324, 952, 2176, 232, 1151, 1343, 467, 1000, 2476, 1318, 1794, 2487, 935, 2139, 2347, 1860, 1746, 1718, 206, 1030, 2330, 111, 1211, 2496, 57, 1669, 1922, 14, 1874, 1002, 1111, 2241, 2092, 1508, 2524, 1162, 2055, 1186, 577, 1770, 2561, 752, 1676, 2041, 2625, 1907, 1726, 255, 258, 886, 2594, 1475, 1885, 2428, 2493, 2497, 710, 761, 2163, 514, 1080, 208, 387, 2044, 2603, 2451, 99, 1535, 1679, 895, 2353, 2073, 894, 1256, 1510, 1940, 2511, 1805, 1583, 1112, 1093, 1833, 1884, 1357, 907, 1712, 2269, 1493, 2053, 1360, 2205, 1075, 817, 820, 1606, 1067, 1161, 2687, 1113, 2129, 2518, 1947, 2558, 149, 1308, 2013, 297, 1740, 1664, 2334, 2528, 1552, 2089, 954, 1300, 953, 2026, 2273, 306, 1130, 440, 336, 2331, 2445, 1371, 657, 191, 2691, 134, 200, 1041, 42, 1133, 147, 334, 740, 2488, 2667, 1064, 353, 2160, 453, 625, 288, 1309, 278, 116, 2118, 1004, 2414, 1175, 1912, 748, 187, 202, 2417, 2130, 1079, 2698, 1074, 1028, 1158, 1240, 1582, 2106, 1453, 2003, 2140, 1197, 1519, 408, 2478, 349, 1640, 422, 2619, 882, 1804, 419, 359, 1367, 2514, 19, 1024, 138, 932, 841, 1783, 1845, 1803, 1498, 1027, 1136, 1209, 2200, 1853, 550, 1611, 1057, 850, 2244, 2012, 1238, 2125, 1925, 1576, 1951, 1477, 1773, 1793, 2067, 1469, 399, 2522, 934, 1255, 1778, 2057, 2564, 18, 1831, 437, 203, 1291, 1849, 1856, 1801, 2703, 955, 2350, 345, 1473, 888, 1044, 1264, 2507, 1786, 2165, 1091, 1330, 1988, 1672, 1544, 1685, 2370, 1424, 1818, 1236, 1496, 1543, 339, 1585, 1125, 1924, 968, 1455, 1224, 1852, 1575, 2382, 1774, 1975, 2095, 1140, 564, 414, 2033, 386, 2248, 2670, 462, 943, 1402, 1829, 389, 924, 2202, 819, 263, 1159, 942, 1219, 1890, 2097, 443, 1284, 2156, 2272, 1248, 835, 2277, 777, 2154, 2046, 225, 925, 77, 1752, 2035, 958, 2695, 2141, 1693, 1509, 1525, 786, 223, 1289, 284, 29, 2032, 1273, 1574, 1281, 1937, 180, 2483, 126, 1096, 2519, 2151, 1187, 988, 1452, 239, 1545, 2279, 2418, 2236, 1088, 2605, 2039, 2447, 1060, 580, 1054, 2671, 240, 2572, 153, 1348, 1846, 1736, 2281, 1713, 2111, 1397, 1454, 1971, 1359, 1098, 429, 118, 1990, 194, 1656, 1695, 849, 2354, 992, 672, 1365, 2245, 1421, 2323, 1903, 1345, 2574, 2078, 2031, 1077, 1751, 75, 1618, 551, 2516, 799, 1931, 1738, 2689, 2027, 1276, 2446, 2617, 2512, 2706, 237, 825, 305, 2404, 1298, 1478, 1542, 705, 726, 2286, 2585, 1066, 1188, 918, 2523, 558, 1009, 495, 2132, 1083, 2015, 2586, 715, 368, 2537, 1772, 10, 1394, 1346, 2346, 1234, 1921, 1827, 1129, 1223, 3, 1919, 772, 807, 1977, 1732, 1832, 2357, 1184, 410, 717, 1680, 1904, 1952, 532, 712, 1242, 227, 2700, 1145, 622, 1370, 1822, 2606, 1534, 537, 1014, 544, 883, 2640, 1283, 911, 2114, 2308, 2647, 2034, 651, 1799, 566, 2526, 1719, 33, 81, 627, 1285, 1608, 1195, 1906, 412, 1244, 1869, 1978, 2659, 515, 2296, 2440, 251, 1550, 1006, 2000, 1858, 1950, 2242, 2631, 2517, 518, 1996, 987, 1963, 1322, 1464, 2063, 2587, 122, 2430, 52, 1063, 1199, 2602, 1114, 66, 504, 1287, 1355, 1316, 2590, 144, 965, 71, 2578, 1353, 2180, 222, 758, 860, 1956, 928, 676, 2102, 642, 982, 209, 1451, 1614, 1340, 293, 2128, 492, 610, 1599, 1404, 2258, 1090, 1756, 2435, 26, 929, 2021, 41, 434, 844, 2247, 2280, 1895, 656, 2707, 576, 2596, 1941, 2126, 809, 1169, 1935, 621, 2109, 457, 1212, 2479, 979, 1724, 2288, 398, 270, 2301, 94, 2646, 2011, 2091, 556, 1092, 2569, 1085, 45, 210, 2675, 1297, 1601, 2464, 454, 1622, 1650, 867, 866, 815, 2618, 1210, 670, 1152, 1521, 2433, 2025, 1259, 1696, 498, 2485, 973, 1908, 411, 1050, 1662, 535, 2305, 355, 991, 2589, 2185, 1325, 1332, 1619, 789, 1385, 2006, 1723, 1299, 39, 1105, 86, 377, 1930, 2007, 2527, 455, 370, 2283, 845, 2355, 1900, 572, 623, 607, 174, 285, 1038, 1729, 2701, 241, 2282, 177, 545, 100, 1331, 1500, 1989, 231, 1667, 1716, 2080, 1839, 2697, 951, 2103, 1415, 426, 960, 2637, 260, 1812, 2306, 274, 1229, 764, 2634, 2149, 1596, 1329, 37, 1434, 2107, 2098, 1757, 2221, 873, 1194, 92, 959, 1246, 874, 1842, 1040, 609, 161, 487, 2581, 919, 1076, 2405, 2029, 1739, 360, 768, 773, 800, 1081, 1176, 2506, 1466, 2048, 2502, 346, 1228, 1071, 2310, 2250, 2224, 130, 618, 218, 2351, 2679, 1862, 1368, 448, 2104, 2144, 1899, 2358, 391, 1183, 27, 666, 1425, 1107, 1457, 1657, 902, 365, 750, 1150, 2345, 1959, 557, 1837, 485, 1984, 2536, 1072, 640, 2207, 1324, 1777, 1646, 1127, 2402, 995, 976, 1418, 821, 20, 769, 1661, 2642, 912, 1490, 1487, 1639, 1520, 458, 1393, 905, 1945, 2122, 310, 1104, 2576, 1139, 1257, 2371, 588, 711, 590, 804, 1825, 2599, 684, 1677, 636, 562, 1386, 1428, 2436, 1910, 327, 2395, 650, 1288, 189, 2534, 1250, 585, 1691, 1758, 851, 1123, 2023, 1566, 1417, 1471, 424, 782, 2442, 415, 1277, 910, 1307, 770, 1692, 1051, 2222, 1110, 502, 539, 1258, 501, 110, 1391, 1414, 1089, 1467, 1710, 1488, 906, 286, 2421, 559, 1350, 1568, 1563, 2182, 2427, 2499, 1815, 320, 2124, 2375, 1486, 766, 1795, 2232, 909, 1513, 1174, 1494, 1810, 2292, 356, 869, 1280, 824, 1636, 647, 1802, 1456, 254, 1607, 2020, 896, 2413, 2256, 2456, 731, 433, 2240, 861, 915, 1877, 1939, 690, 996, 547, 2317, 271, 1482, 704, 1844, 2639, 620, 2532, 2322, 97, 1249, 553, 1001, 1728, 1560, 765, 2544, 1073, 1706, 687, 1807, 763, 1867, 2291, 1168, 1419, 1031, 1748, 34, 2597, 107, 314, 775, 397, 1099, 2275, 2199, 1785, 1967, 1980, 390, 2260, 1782, 129, 1062, 2684, 480, 275, 638, 528, 1722, 2471, 233, 1327, 1771, 1144, 2332, 1570, 2665, 2629, 490, 658, 2172, 120, 12, 2266, 1301, 608, 1865, 9, 220, 1684, 1384, 2455, 855, 2108, 2582, 88, 2285, 563, 2070, 2416, 1333, 329, 759, 822, 834, 1518, 1731, 54, 2074, 1116, 1351, 1193, 2198, 135, 1190, 1866, 1943, 162, 238, 503, 1481, 207, 1048, 2237, 309, 1137, 692, 1106, 797, 725, 395, 1609, 235, 95, 1046, 1446, 2209, 2555, 40, 852, 626, 1305, 1663, 2146, 864, 1567, 599, 491, 2636, 2672, 1378, 1388, 143, 791, 420, 1059, 287, 734, 957, 1196, 2131, 2204, 2425, 1549, 1354, 1304, 1887, 771, 1817, 2503, 683, 1173, 259, 1617, 2259, 1876, 2233, 541, 700, 2364, 2556, 2623, 154, 1239, 381, 2468, 695, 2085, 587, 507, 603, 367, 1205, 2228, 396, 785, 956, 1086, 1274, 486, 289, 2086, 1361, 1902, 102, 1670, 2121, 379, 1598, 1363, 1683, 2201, 2356, 466, 776, 806, 833, 783, 1577, 2510, 1994, 1537, 2311, 810, 2565, 2538, 1942, 2052, 823, 2652, 2195, 1458, 1375, 1742, 101, 802, 1056, 1958, 2150, 2197, 115, 49, 2458, 2505, 402, 477, 1334, 1784, 1341, 400, 2491, 2550, 570, 516, 643, 813, 1828, 1290, 1830, 1045, 244, 1946, 164, 500, 1824, 1848, 1976, 1649, 1233, 1911, 152, 856, 1119, 2344, 1143, 267, 1142, 326, 1759, 1282, 795, 2400, 361, 842, 793, 985, 2621, 2087, 2155, 1313, 2422, 1443, 745, 1179, 2042, 1230, 2246, 1253, 2268, 1893, 616, 1410, 1501, 2276, 1406, 2075, 1335, 1235, 574, 1587, 571, 330, 673, 1243, 612, 594, 686, 2022, 1095, 858, 2225, 2592, 405, 513, 1668, 90, 445, 1376, 1882, 2615, 2262, 862, 256, 975, 1553, 2112, 1881, 2432, 1213, 1531, 1970, 2191, 693, 922, 1538, 104, 880, 1007, 2389, 595, 84, 2396, 1721, 1010, 2682, 615, 1015, 1069, 972, 2660, 1652, 165, 722, 2549, 1651, 483, 2398, 2562, 2694, 967, 2509, 51, 291, 548, 1251, 362, 644, 105, 1400, 1447, 568, 510, 2548, 1121, 729, 1448, 1247, 2325, 2543, 1623, 313, 421, 2051, 2448, 1023, 1155, 1923, 2388, 1306, 1602, 2498, 230, 2494, 328, 743, 1613, 73, 2437, 653, 2520, 2110, 741, 1861, 427, 508, 511, 2298, 425, 494, 735, 573, 2313, 428, 746, 668, 679, 2016, 2560, 2638, 1185, 55, 308, 1897, 1965, 403, 2309, 1819, 971, 581, 1580, 2379, 1362, 1709, 109, 526, 2475, 2580, 1078, 2212, 193, 1167, 2349, 1445, 246, 1261, 1792, 1573, 1694, 1675, 575, 659, 931, 904, 1438, 2056, 65, 282, 318, 1241, 151, 106, 146, 171, 853, 1403, 1460, 2360, 186, 4, 560, 1026, 601, 1034, 1527, 1769, 124, 128, 2664, 2337, 2297, 137, 1043, 2426, 2547, 2557, 1938, 661, 634, 1474, 2147, 132, 2329, 1033, 315, 98, 1383, 430, 1222, 2213, 2656, 1523, 1087, 2552, 2441, 23, 217, 784, 843, 2254, 1779, 2096, 2438, 1790, 347, 2327, 1182, 2019, 505, 1265, 136, 157, 1514, 1737, 2568, 296, 1423, 2171, 108, 166, 629, 1206, 2466, 2669, 2316, 2274, 1992, 103, 1120, 2138, 2249, 248, 1838, 323, 140, 316, 887, 655, 2314, 1439, 344, 754, 635, 2116, 792, 1985, 2220, 2477, 76, 493, 1405, 876, 375, 63, 1097, 1590, 2208, 2495, 2335, 1170, 1851, 1435, 1412, 1735, 703, 261, 664, 533, 538, 1505, 438, 24, 586, 1823, 2386, 780, 554, 317, 1603, 1302, 632, 2390, 920, 1154, 22, 1541, 382, 796, 1747, 2321, 342, 2227, 2217, 721, 1429, 1780, 2083, 277, 121, 85, 472, 1961, 2287, 1745, 2553, 2577, 1431, 1625, 2218, 2361, 50, 827, 737, 1373, 470, 1430, 978, 727, 671, 2567, 1547, 836, 1427, 645, 1854, 417, 199, 2484, 663, 1597, 364, 697, 2153, 114, 1688, 2465, 2134, 2359, 2515, 1160, 2145, 283, 1889, 2148, 604, 2683, 2307, 96, 641, 2384, 2210, 591, 1901, 2419, 303, 228, 482, 1337, 2143, 1539, 1727, 774, 790, 2403, 875, 450, 530, 983, 688, 2367, 2423, 1374, 1189, 182, 1227, 1878, 1917, 2657, 2474, 2607, 582, 2686, 358, 1966, 2188, 64, 2500, 212, 369, 881, 1767, 216, 2315, 2469, 1796, 1687, 168, 452, 1172, 1442, 78, 2450, 1303, 2431, 1826, 348, 903, 2372, 1872, 788, 1628, 2579, 2229, 698, 392, 1891, 1503, 133, 521, 2394, 2271, 2005, 865, 1918, 1278, 2462, 80, 816, 444, 1689, 1850, 600, 713, 307, 646, 756, 914, 311, 1319, 148, 1734, 1800, 1198, 744, 197, 2626, 854, 667, 2609, 714, 1202, 1600, 28, 229, 351, 1321, 60, 1883, 2490, 413, 2641, 127, 950, 617, 520, 1061, 479, 1013, 723, 338, 2076, 1468, 1392, 404, 524, 354, 2504, 292, 1165, 1177, 173, 372, 1126, 70, 2004, 2263, 1855, 2482, 125, 179, 2508, 829, 499, 181, 1954, 459, 1049, 2088, 2119, 1315, 451, 868, 2231, 1743, 689, 1055, 401, 2162, 583, 2489, 1349, 1459, 1037, 247, 857, 755, 1750, 471, 760, 1776, 2391, 6, 2312, 2017, 1506, 665, 170, 219, 2453, 262, 335, 512, 300, 224, 660, 2290, 1356, 519, 1898, 1700, 1875, 2158, 441, 93, 2060, 195, 2289, 1232, 1654, 1841, 2008, 669, 897, 298, 304, 1480, 2028, 2393, 549, 718, 1231, 1616, 281, 1201, 1632, 652, 385, 423, 431, 878, 2267, 916, 1411, 1342, 332, 637, 357, 542, 2598, 1960, 649, 871, 531, 1936, 1216, 2542, 156, 901, 1377, 1847, 2501, 337, 2551, 742, 2473, 2190, 2449, 540, 2064, 2622, 593, 1398, 1502, 465, 2040, 1109, 1181, 478, 1382, 266, 272, 2142, 1754, 648, 2702, 561, 333, 406, 2047, 1660, 2392, 2486, 801, 2049, 811, 628, 439, 1581, 1021, 363, 2365, 639, 435, 846, 1974, 678, 2645, 319, 859, 739, 757, 21, 25, 322, 2604, 432, 1811, 1905, 681, 242, 142, 1328, 1816, 1702, 2157, 981, 214, 613, 946, 371, 838, 1591, 269, 707, 17, 343, 198, 117, 605, 592, 1798, 2120, 1561, 619, 794, 552, 1200, 2554, 840, 2573, 1806, 2084, 1645, 1944, 2068, 2320, 1703, 999, 340, 384, 2533, 2, 46, 584, 1594, 47, 2663, 2113, 680, 1157, 1562, 1108, 2038, 724, 694, 2662, 469, 2399, 268, 2531, 730, 176, 1886, 2302, 497, 569, 1814, 2009, 205, 257, 803, 677, 2170, 908, 787, 738, 2704, 2002, 302, 1483, 2251, 321, 2295, 1730, 685, 1395, 1166, 89, 1326, 1192, 366, 394, 767, 546, 674, 460, 828, 818, 2387, 1441, 1389, 2661, 325, 1789, 264, 762, 2219, 2460, 2452, 383, 1704, 523, 716, 719, 1449, 1472, 2401, 2521, 779, 751, 1540, 1648, 606, 213, 1008, 1797, 2270, 2374, 1605, 2651, 226, 589, 456, 215, 1949, 2293, 2429, 1969, 611, 555, 2529, 185, 720, 994, 447, 253, 83, 749, 167, 2338, 2077, 579, 948, 1450, 736, 352, 2472, 1320, 1871, 1913, 489, 691, 1117, 2470, 2211, 733, 1715, 701, 1204, 1132, 941, 250, 1156, 2072, 2161, 2420, 2299, 295, 188, 236, 1955, 527, 525, 624, 682, 578, 602, 675, 2336, 294, 654, 252, 407, 418, 468, 484, 473, 1381, 474, 2584, 350, 863, 341, 1644, 732, 416, 708, 1042, 2593, 119, 805, 826, 380, 1336, 113, 169, 812, 522, 2444, 706, 488, 814, 900, 2168, 728, 847, 747, 91, 870, 837, 280, 211, 374, 1146, 662, 2658, 702, 2535, 7, 234, 476, 2018, 630, 139, 798, 1102, 1191, 1682, 2632, 2187, 596, 879, 567, 631, 72, 2397, 2066, 2443, 82, 1546, 48, 872]\n"
     ]
    }
   ],
   "source": [
    "# difficulty aware selection\n",
    "\n",
    "import networkx as nx\n",
    "from torch_geometric.datasets import Planetoid\n",
    "from torch_geometric.utils import to_networkx\n",
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "from torch_geometric.datasets import Planetoid\n",
    "from sklearn.metrics.pairwise import euclidean_distances\n",
    "\n",
    "\n",
    "def get_rank_score(rank_list,n):\n",
    "    result=[0 for i in range(n)]\n",
    "    for i in range(n):\n",
    "        k=rank_list[i]\n",
    "        result[k]=100*(n-i)/n\n",
    "    return result\n",
    "\n",
    "def compute_pagerank_and_ranking(data):\n",
    "    \"\"\"\n",
    "    page rank ranking\n",
    "    \"\"\"\n",
    "    G = to_networkx(data, to_undirected=True)  \n",
    "\n",
    "\n",
    "    pagerank = nx.pagerank(G, alpha=0.85)\n",
    "\n",
    " \n",
    "    sorted_nodes = sorted(pagerank.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "  \n",
    "    rank_list = [node for node, rank in sorted_nodes]\n",
    "\n",
    "    return get_rank_score(rank_list,data.num_nodes)\n",
    "import faiss\n",
    "from torch_geometric.utils import to_undirected\n",
    "\n",
    "\n",
    "def compute_rC_density(node_features: torch.Tensor, cluster_centers: torch.Tensor, labels: torch.Tensor):\n",
    "\n",
    "    assigned_centers = cluster_centers[labels]       \n",
    "    distances = torch.norm(node_features - assigned_centers, dim=1)  \n",
    "    c_density = 1 / (1 + distances)                   \n",
    "    return c_density\n",
    "\n",
    "\n",
    "def compute_cluster_rank(data, num_clusters=7):\n",
    "\n",
    "\n",
    "    x = data.x.detach().cpu().float()\n",
    "    n, d = x.shape\n",
    "\n",
    "  \n",
    "    kmeans = faiss.Kmeans(d=d, k=num_clusters, niter=20, verbose=False, seed=42)\n",
    "    kmeans.train(x.numpy())\n",
    "\n",
    "    \n",
    "    _, I = kmeans.index.search(x.numpy(), 1)  \n",
    "    labels = torch.tensor(I.squeeze(), dtype=torch.long) \n",
    "\n",
    "    centers = torch.tensor(kmeans.centroids, dtype=x.dtype) \n",
    "\n",
    "    c_density = compute_rC_density(x, centers, labels) \n",
    "\n",
    "    sorted_idx = torch.argsort(c_density, descending=True)  \n",
    "\n",
    "    rank_list = sorted_idx.tolist()\n",
    "    return get_rank_score(rank_list, data.num_nodes)\n",
    "\n",
    "\n",
    "\n",
    "n=data_train.num_nodes\n",
    "#print(n)\n",
    "page_list=compute_pagerank_and_ranking(data_train)\n",
    "cluster_list=compute_cluster_rank(data_train)\n",
    "\n",
    "\n",
    "result_score=[]\n",
    "for i in range(n):\n",
    "    score=page_list[i]*0.5+cluster_list[i]*0.5\n",
    "    result_score.append((i,score))\n",
    "\n",
    "#print(result_score)\n",
    "\n",
    "sorted_result=sorted(result_score, key=lambda x: x[1], reverse=True)\n",
    "rank_list = [node for node, _ in sorted_result]\n",
    "#rank_list = rank_list[::-1]\n",
    "print(rank_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "608\n",
      "0.8898026315789473\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "filter_list=[]\n",
    "total=0\n",
    "op=0\n",
    "for i in rank_list:\n",
    "    if LLM_label['conf'][i].item()>=0.88:\n",
    "        total+=1\n",
    "        filter_list.append(i)\n",
    "        if LLM_label['pred'][i].item()==data_org.y[i].item():\n",
    "            op+=1\n",
    "print(len(filter_list))\n",
    "print(op/total)\n",
    "\"\"\"\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
